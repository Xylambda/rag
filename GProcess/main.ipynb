{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial from: https://www.youtube.com/watch?v=q9MD_hU2Yd8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "We first load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to load 1 document only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages = []\n",
    "\n",
    "pdf_files = list(Path('docs').iterdir())\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    pages = loader.load()\n",
    "    all_pages.extend(pages)  # Combine pages from all PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom langchain_community.document_loaders import PyPDFLoader\\n\\nloader = PyPDFLoader('docs/gaussian_process.pdf')\\npages = loader.load()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('docs/gaussian_process.pdf')\n",
    "pages = loader.load()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the context window of a LLM is limited we need to break down the documents into chunks.\n",
    "\n",
    "We split such that every chunk uses a little bit of content of the previous chunk. We do that to avoid the model having incomplete information in each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_texts = []\n",
    "for doc in all_pages:\n",
    "    chunks = splitter.split_documents([doc])  # Split each document separately\n",
    "    split_texts.extend(chunks)  # Combine the split chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Store data\n",
    "\n",
    "We use a VectorDB to store the chunks. We previously have to generate embeddings for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Store embeddings in ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create the client of Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(split_texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retriever\n",
    "\n",
    "How do I get data from my Vector store? We need to create a retriever object.\n",
    "\n",
    "The retriever transforms the question into a vector and retrieves similar chunks from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant chunks for the passed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 3, 'source': 'docs/g_process.pdf'}, page_content='“Regularization and Model Selection”).\\nIn the case of Bayesian linear regression, however, the inte grals actually are tractable! In\\nparticular, for Bayesian linear regression, one can show (a fter much work!) that\\nθ|S∼ N(1\\nσ2A−1XT⃗ y, A−1)\\ny∗|x∗, S∼ N(1\\nσ2xT\\n∗A−1XT⃗ y, xT\\n∗A−1x∗+σ2)\\n4'),\n",
       " Document(metadata={'page': 12, 'source': 'docs/g_process.pdf'}, page_content='p(xA|xB) =1∫\\nxAp(xA, xB;µ,Σ)dxA·[1\\n(2π)m/2|Σ|1/2exp(\\n−1\\n2(x−µ)TΣ−1(x−µ))]\\n(4)\\n=1\\nZ1exp{\\n−1\\n2([xA−µA\\nxB−µB])T[VAAVAB\\nVBAVBB][xA−µA\\nxB−µB]}\\n(5)\\n13'),\n",
       " Document(metadata={'page': 10, 'source': 'docs/g_process.pdf'}, page_content='1. As Bayesian methods, Gaussian process models allow one to quantify uncertainty in\\npredictions resulting not just from intrinsic noise in the p roblem but also the errors\\nin the parameter estimation procedure. Furthermore, many m ethods for model selec-\\ntion and hyperparameter selection in Bayesian methods are i mmediately applicable to\\nGaussian processes (though we did not address any of these ad vanced topics here).\\n2. Like locally-weighted linear regression, Gaussian proc ess regression is non-parametric\\nand hence can model essentially arbitrary functions of the i nput points.\\n3. Gaussian process regression models provide a natural way to introduce kernels into a\\nregression modeling framework. By careful choice of kernel s, Gaussian process regres-\\nsion models can sometimes take advantage of structure in the data (though, we also\\ndid not examine this issue here).\\n4. Gaussian process regression models, though perhaps some what tricky to understand\\nconceptually, nonetheless lead to simple and straightforw ard linear algebra implemen-\\ntations.\\nReferences\\n[1] Carl E. Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine\\nLearning. MIT Press, 2006. Online: http://www.gaussianprocess.org/gpml/\\n11'),\n",
       " Document(metadata={'page': 8, 'source': 'docs/g_process.pdf'}, page_content='⃗f∗∈Rm∗such that ⃗f∗=[\\nf(x(1)\\n∗)· · ·f(x(m)\\n∗)]T\\nK(X, X)∈Rm×msuch that ( K(X, X))ij=k(x(i), x(j))\\nK(X, X ∗)∈Rm×m∗such that ( K(X, X ∗))ij=k(x(i), x(j)\\n∗)\\nK(X∗, X)∈Rm∗×msuch that ( K(X∗, X))ij=k(x(i)\\n∗, x(j))\\nK(X∗, X∗)∈Rm∗×m∗such that ( K(X∗, X∗))ij=k(x(i)\\n∗, x(j)\\n∗).\\nFrom our i.i.d. noise assumption, we have that\\n[\\n⃗ ε\\n⃗ ε∗]\\n∼ N(\\n⃗0,[\\nσ2I⃗0\\n⃗0Tσ2I])\\n.\\n10We assume also that TareSare mutually independent.\\n9')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Normal distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature is basically telling the model how creative it needs to be. Zero is lowest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=MODEL_NAME, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The President of Spain is currently Pedro Sánchez. He has been serving as Prime Minister (not President, actually) since June 2018 and leader of the Spanish Socialist Workers' Party (PSOE). However, the head of state in Spain is a monarch, known as the King or Queen of Spain.\\n\\nAs of my last update, the current monarch is King Felipe VI. He has been serving as the King of Spain since June 2014, following the abdication of his father, King Juan Carlos I.\\n\\nSo, to clarify:\\n\\n* The head of government (Prime Minister) in Spain is Pedro Sánchez.\\n* The head of state (King or Queen) in Spain is King Felipe VI.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-09-29T22:51:40.589182375Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2484382511, 'load_duration': 25705161, 'prompt_eval_count': 17, 'prompt_eval_duration': 41256000, 'eval_count': 144, 'eval_duration': 2376099000}, id='run-5ad39218-16b3-44ca-818b-b2b5453f3435-0', usage_metadata={'input_tokens': 17, 'output_tokens': 144, 'total_tokens': 161})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Who is the president of Spain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Parsing model response\n",
    "\n",
    "We do not consume the answer from the model. Instead, we sent it to a parser first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pipe the model with the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of my last update in April 2023, Pedro Sánchez Pérez-Castejón has been serving as the Prime Minister (Presidente del Gobierno) of Spain since June 2018. He leads a government that is part of the Spanish Socialist Workers' Party (PSOE). However, please note that political leadership can change over time due to elections or other factors. For the most current information, I recommend checking reputable news sources or official government websites for updates on the Prime Minister of Spain.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who is the prime minister of Spain?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also provide the context from the documents. We are going to use a PromptTemplate do to so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the prompt to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final App\n",
    "\n",
    "Putting all together, we need to add the retriever to the chain (pipe). The retriever will get the question and it will output a context (obtained from the vector store) for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Explain Gaussian processes in a nutshell\",\n",
    "    \"Why do we use a Normal distribution?\",\n",
    "    \"What does the covariance matrix has to do with kernels?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A flexible framework for regression that models uncertain observations and can be used to perform tasks like linear regression, numerical integration, and solving differential equations. It's based on multivariate Gaussian distributions and can be adapted for different purposes.\n",
      "******************************\n",
      "Because it's based on the notion of the Gaussian distribution (normal distribution), named after Carl Friedrich Gauss.\n",
      "******************************\n",
      "The covariance matrix parameter is the Gram matrix of your N points with some desired kernel.\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for question in questions:\n",
    "    answer = chain.invoke(\n",
    "        {'question': question}\n",
    "    )\n",
    "    answers.append(answer)\n",
    "    print(answer)\n",
    "    print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. Load data and split it into chunks.\n",
    "2. Save chunks into a VectorDB:\n",
    "    2.1. Generate embeddings.\n",
    "    2.2. Create a retriever for the VectorDB.\n",
    "3. Create a model.\n",
    "4. Create parser.\n",
    "5. Create a template prompt and a prompt.\n",
    "6. Create a pipe with everything."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
